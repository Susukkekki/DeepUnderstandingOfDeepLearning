{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Overfitting, cross-validation, regularization\n",
    "### LECTURE: Cross-validation on regression\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202305"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Susukkekki/DeepUnderstandingOfDeepLearning/blob/main/overfitting/DUDL_overfitting_regression.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j7-LiwqUMGYL",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wz6w7TtgQ6QF"
   },
   "source": [
    "# Create the data and the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j-SP8NPsMNRL",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "N = 100\n",
    "x = torch.randn(N,1)\n",
    "y = x + torch.randn(N,1)\n",
    "\n",
    "# and plot\n",
    "plt.plot(x,y,'s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krQeh5wYMNla",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# build model\n",
    "ANNreg = nn.Sequential(\n",
    "    nn.Linear(1,1),  # input layer\n",
    "    nn.ReLU(),       # activation function\n",
    "    nn.Linear(1,1)   # output layer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmHh7GrvMNoy",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# model meta-parameters\n",
    "\n",
    "learningRate = .05\n",
    "\n",
    "# loss function\n",
    "lossfun = nn.MSELoss()\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.SGD(ANNreg.parameters(),lr=learningRate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "vpZsJzRKQ-xM"
   },
   "source": [
    "# Select data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "of9E8ClxMNsD",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# select training data (note the hard-coded N!)\n",
    "trainidx  = np.random.choice(range(N),80,replace=False) # random indices\n",
    "trainBool = np.zeros(N,dtype=bool) # initialize vector of Falses'\n",
    "trainBool[trainidx] = True # set selected samples to True\n",
    "\n",
    "# show the sizes\n",
    "print(x[trainBool].shape)\n",
    "print(x[~trainBool].shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lmUXAALTRPkL"
   },
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EY4ayy2VRGeZ",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "numepochs = 500\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # forward pass\n",
    "  yHat = ANNreg(x[trainBool])\n",
    "\n",
    "  # compute loss\n",
    "  loss = lossfun(yHat,y[trainBool])\n",
    "\n",
    "  # backprop\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmX6K49WMNuy",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# report the losses\n",
    "\n",
    "# compute losses of the TEST set\n",
    "predYtest = ANNreg(x[~trainBool])\n",
    "testloss = (predYtest-y[~trainBool]).pow(2).mean()\n",
    "\n",
    "# print out final TRAIN loss and TEST loss\n",
    "print(f'Final TRAIN loss: {loss.detach():.2f}')\n",
    "print(f'Final TEST loss: {testloss.detach():.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i1TCt0mpMNxC",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "### plot the data\n",
    "\n",
    "# predictions for final training run\n",
    "predYtrain = ANNreg(x[trainBool]).detach().numpy()\n",
    "\n",
    "# now plot\n",
    "plt.plot(x,y,'k^',label='All data')\n",
    "plt.plot(x[trainBool], predYtrain,\n",
    "         'bs',markerfacecolor='w',label='Training pred.')\n",
    "plt.plot(x[~trainBool],predYtest.detach(),\n",
    "         'ro',markerfacecolor='w',label='Test pred.')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucAVWWYEbBE5",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "jjvdKsrdZ4ka"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5EPeV1KZZ42b",
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# 1) The train/test split is currently hard-coded to be 80/20 (note the number \"80\"). This is bad coding style, because\n",
    "#    if you change the number of datapoints from N=100 to N=10000, then we're still only training on 80 samples and testing\n",
    "#    on 10000-80=9920 samples. Change how the variable trainBool is created so that it always trains on 80% of the data,\n",
    "#    regardless of the dataset size.\n",
    "# \n",
    "# 2) Re-write this code to use scikitlearn and/or DataLoader instead of manually separating the data into train/test.\n",
    "# \n",
    "# 3) Do we really need 500 epochs to train the model? To find out, add code to the training loop to compute the MSEloss \n",
    "#    for the train and test data on each iteration during training. Then plot the train and test error as a function of\n",
    "#    training epoch. What is your evaluation of an appropriate amount of training for this model/dataset?\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNSoqVKdJX7oDTOd7ULis2C",
   "collapsed_sections": [],
   "name": "DUDL_overfitting_regression.ipynb",
   "provenance": [
    {
     "file_id": "1Q_oDw0aMA4QFKDnLxuqJp62P8oPMtO1R",
     "timestamp": 1616705543155
    },
    {
     "file_id": "1FtQ99beHYcDFDywLdaPgFm-KjBeI8PvD",
     "timestamp": 1615884593383
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
